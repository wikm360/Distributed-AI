// lib/chat_screen.dart
import 'dart:async';
import 'dart:convert';
import 'package:flutter/material.dart';
import 'package:llama_cpp_dart/llama_cpp_dart.dart';
import 'package:http/http.dart' as http;

// Custom format for Qwen models
class QwenFormat extends PromptFormat {
  QwenFormat()
      : super(
          PromptFormatType.raw,
          inputSequence: '<|im_start|>user\n',
          outputSequence: '<|im_start|>assistant\n',
          systemSequence: '<|im_start|>system\n',
          stopSequence: '<|im_end|>',
        );
}

class ChatScreen extends StatefulWidget {
  final String libPath;
  final String modelPath;
  const ChatScreen({Key? key, required this.libPath, required this.modelPath}) : super(key: key);

  @override
  _ChatScreenState createState() => _ChatScreenState();
}

class _ChatScreenState extends State<ChatScreen> {
  late LlamaParent llamaParent;
  final List<ChatMessage> _messages = [];
  final TextEditingController _controller = TextEditingController();
  bool _isTyping = false;

  // ğŸ” Worker Timer
  late Timer _workerTimer;

  // âœ… Set of query numbers already answered
  final Set<int> _answeredQueries = <int>{};

  // ğŸ“¡ Routing server URL
  static const String routingServerUrl = "http://85.133.228.31:8313";

  @override
  void initState() {
    super.initState();
    initModel();
    startBackgroundWorker();
  }

  Future<void> initModel() async {
    try {
      Llama.libraryPath = widget.libPath;

      final contextParams = ContextParams()
        ..nCtx = 4096
        ..nBatch = 512
        ..nThreads = 8
        ..nPredict = 128;

      final samplerParams = SamplerParams()
        ..temp = 0.6
        ..topP = 0.9
        ..topK = 50;

      final loadCommand = LlamaLoad(
        path: widget.modelPath,
        modelParams: ModelParams(),
        contextParams: contextParams,
        samplingParams: samplerParams,
        // format: ChatMLFormat(),
        format: QwenFormat(),
      );

      llamaParent = LlamaParent(loadCommand);
      await llamaParent.init();

      int attempts = 0;
      while (llamaParent.status != LlamaStatus.ready && attempts < 30) {
        await Future.delayed(const Duration(milliseconds: 500));
        attempts++;
      }

      if (llamaParent.status != LlamaStatus.ready) {
        addSystemMessage("âŒ Model failed to load.");
        return;
      }

      print("[INFO] âœ… Model loaded successfully.");

      // âœ… Global listener for completions
      llamaParent.completions.listen((event) {
        if (event.success) {
          print("[INFO] âœ… Response generation completed successfully.");
        } else {
          print("[ERROR] âŒ Response generation failed: ${event.promptId}");
        }
      });
    } catch (e) {
      addSystemMessage("âŒ Model initialization error: $e");
      print("[ERROR] âŒ initModel: $e");
    }
  }

  void addSystemMessage(String text) {
    setState(() {
      _messages.add(ChatMessage(text: text, isFromUser: false));
    });
  }

  // âœ… Start background worker â€” checks every 3 seconds
  void startBackgroundWorker() {
    _workerTimer = Timer.periodic(const Duration(seconds: 3), (timer) async {
      if (!mounted) return;

      try {
        final response = await http.get(Uri.parse('$routingServerUrl/request'));
        if (response.statusCode == 200) {
          final List<dynamic> queries = jsonDecode(response.body);
          if (queries.isEmpty) return;

          for (var queryData in queries) {
            final String query = queryData['query'];
            final int queryNumber = queryData['query_number'];

            if (_answeredQueries.contains(queryNumber)) continue;

            print("[Worker] ğŸš€ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©ÙˆØ¦Ø±ÛŒ $queryNumber: ${query.substring(0, 50)}...");

            // âœ… Ø¹Ù„Ø§Ù…Øª Ø²Ø¯Ù† Ù¾Ø§Ø³Ø® Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡
            _answeredQueries.add(queryNumber);

            try {
              final prompt = '''
<|im_start|>system
You are a helpful, concise AI assistant. Answer shortly and directly.<|im_end|>
<|im_start|>user
$query<|im_end|>
<|im_start|>assistant
  ''';
// <start_of_turn>user
// You are a helpful, concise assistant. Please answer the following query:
// $query<end_of_turn>
// <start_of_turn>model

              // âœ… Ø§Ø³ØªØ±ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯ Ø¯Ø± ØªØ±Ù…ÛŒÙ†Ø§Ù„
              final StringBuffer buffer = StringBuffer();
              late StreamSubscription completionSub;

              final streamSub = llamaParent.stream.listen(
                (token) {
                  buffer.write(token);
                  print("[Worker Token] $token");
                },
                onError: (e) => print("[Worker] âŒ Ø§Ø³ØªØ±ÛŒÙ… Ø®Ø·Ø§ Ø¯Ø§Ø¯: $e"),
              );

              completionSub = llamaParent.completions.listen((event) async {
                if (event.success) {
                  final answer = buffer.toString().trim();
                  final success = await sendResponseToServer(queryNumber, answer);
                  if (success) {
                    print("[Worker] âœ… Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ Ú©ÙˆØ¦Ø±ÛŒ $queryNumber Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯.");
                  }
                } else {
                  print("[Worker] âŒ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯.");
                }
                await streamSub.cancel();
                await completionSub.cancel();
              });

              await llamaParent.sendPrompt(prompt);
            } catch (e) {
              print("[Worker] âŒ Ø®Ø·Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´: $e");
              _answeredQueries.remove(queryNumber);
            }
          }
        }
      } catch (e) {
        // Ø³Ø±ÙˆØ± Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª â€” ÙÙ‚Ø· Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø¯Ù‡
      }
    });
  }

  // âœ… Generate local response using Completer
// âœ… ØªØºÛŒÛŒØ±: Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† useTimeout
Future<String> generateLocalResponse(String query, {bool useTimeout = true}) async {
  print("[Worker] ğŸ§  Starting local response generation for: $query ${useTimeout ? '(with timeout)' : '(no timeout)'}");

  final completer = Completer<String>();
  final StringBuffer buffer = StringBuffer();

  final streamSub = llamaParent.stream.listen(
    (token) {
      buffer.write(token);
      print("[Worker Token] $token");
    },
    onError: (e) {
      if (!completer.isCompleted) {
        completer.completeError(e);
      }
    },
  );

  final completionSub = llamaParent.completions.listen(
    (event) {
      if (event.success && !completer.isCompleted) {
        completer.complete(buffer.toString().trim());
      } else if (!completer.isCompleted) {
        completer.completeError("Response generation failed.");
      }
    },
    onError: (e) {
      if (!completer.isCompleted) {
        completer.completeError(e);
      }
    },
  );

  try {
    final prompt = '''
<|im_start|>system
You are a helpful, concise AI assistant. Answer shortly and directly.<|im_end|>
<|im_start|>user
$query<|im_end|>
<|im_start|>assistant
''';

// <start_of_turn>user
// You are a helpful, concise assistant. Please answer the following query:
// $query<end_of_turn>
// <start_of_turn>model

    print("[Worker] ğŸ“¤ Sending prompt to model...");
    await llamaParent.sendPrompt(prompt);

    // âœ… ÙÙ‚Ø· Ø§Ú¯Ø± useTimeout=true Ø¨Ø§Ø´Ø¯ØŒ ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª Ø§Ø¹Ù…Ø§Ù„ Ø´ÙˆØ¯
    final result = useTimeout
        ? await completer.future.timeout(
            const Duration(seconds: 30),
            onTimeout: () => "âŒ Response timed out.",
          )
        : await completer.future; // Ø¨Ø¯ÙˆÙ† ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª

    await streamSub.cancel();
    await completionSub.cancel();

    print("[Worker] ğŸ“ Final response: $result");
    return result;
  } catch (e) {
    print("[Worker] âŒ Error generating response: $e");
    await streamSub.cancel();
    await completionSub.cancel();
    return "Error: $e";
  }
}

  // Send response to server
  Future<bool> sendResponseToServer(int queryNumber, String answer) async {
    print("[Worker] ğŸ“¤ Sending response to server for query $queryNumber");
    try {
      final response = await http.post(
        Uri.parse('$routingServerUrl/response'),
        headers: {'Content-Type': 'application/json'},
        body: jsonEncode({
          "query_number": queryNumber,
          "response": answer,
        }),
      );

      if (response.statusCode == 200) {
        print("[Worker] âœ… Response sent: $queryNumber");
        return true;
      } else {
        print("[Worker] âŒ Failed to send: ${response.statusCode} - ${response.body}");
        return false;
      }
    } catch (e) {
      print("[Worker] âŒ Send error: $e");
      return false;
    }
  }

  // User sends message
  void _sendMessage(String text) async {
    final input = text.trim();
    if (input.isEmpty || _isTyping) return;

    setState(() {
      _messages.add(ChatMessage(text: input, isFromUser: true));
      _isTyping = true;
      _messages.add(ChatMessage(text: "", isFromUser: false));
    });
    _controller.clear();

    // âœ… Ø§Ø³ØªØ±ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ ØªÙˆÚ©Ù† Ø¨Ù‡ ØªÙˆÚ©Ù† Ø¯Ø± UI
    final subscription = llamaParent.stream.listen(
      (token) {
        if (!mounted) return;
        setState(() {
          if (_isTyping && _messages.isNotEmpty) {
            _messages.last.text += token;
          }
        });
      },
      onError: (e) {
        if (!mounted) return;
        setState(() {
          _messages.last.text += "\n\nâŒ Ø®Ø·Ø§ÛŒ Ù…Ø¯Ù„: $e";
          _isTyping = false;
        });
      },
    );

    try {
      // ğŸ” Ø³Ø¹ÛŒ Ø¯Ø± Ø­Ø§Ù„Øª Ø´Ø¨Ú©Ù‡
      final queryNumber = await submitQuery(input);
      if (queryNumber == null) {
        // ğŸ”½ ÙÛŒÙ„â€ŒØ¨Ú© Ø¨Ù‡ Ø¢ÙÙ„Ø§ÛŒÙ† â€” Ø¨Ø¯ÙˆÙ† ØªØ§ÛŒÙ…â€ŒØ§ÙˆØªØŒ Ø§Ù…Ø§ Ø¨Ø§ Ø§Ø³ØªØ±ÛŒÙ…
        print("[User] ğŸŒ Ø³Ø±ÙˆØ± Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³ØªØŒ Ø­Ø§Ù„Øª Ø¢ÙÙ„Ø§ÛŒÙ† ÙØ¹Ø§Ù„ Ø´Ø¯...");

        final prompt = '''
<|im_start|>system
You are a helpful, concise AI assistant. Answer shortly and directly.<|im_end|>
<|im_start|>user
$input<|im_end|>
<|im_start|>assistant
  ''';

// <start_of_turn>user
// You are a helpful, concise assistant. Please answer the following query:
// $input<end_of_turn>
// <start_of_turn>model

        await llamaParent.sendPrompt(prompt);
        return; // Ù…Ù†ØªØ¸Ø± Ù…ÛŒâ€ŒÙ…Ø§Ù†ÛŒÙ… ØªØ§ `completions` ÛŒØ§ `stream` Ú©Ø§Ø± Ú©Ù†Ø¯
      }

      print("[User] âœ… Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯ØŒ query_number: $queryNumber");

      final workerResponses = await waitForWorkerResponses(queryNumber);
      print("[User] ğŸ“¥ ${workerResponses.length} Ù¾Ø§Ø³Ø® Ø§Ø² Ø¯ÛŒÚ¯Ø± Ù†ÙˆØ¯Ù‡Ø§ Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯");

      final finalPrompt = buildFinalPrompt(input, workerResponses);
      print("[User] ğŸ“ Ù¾Ø±Ø§Ù…Øª Ù†Ù‡Ø§ÛŒÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª.");

      await llamaParent.sendPrompt(finalPrompt);

      await cleanupQuery(queryNumber);
    } catch (e) {
      print("[User] âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ: $e");
      // Ø§Ú¯Ø± Ø­ØªÛŒ Ø§ÛŒÙ† Ù‡Ù… Ø®Ø·Ø§ Ø¯Ø§Ø¯ØŒ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ÙÛŒÙ„â€ŒØ¨Ú©
      final prompt = '''
  <|im_start|>system
  You are a helpful assistant.<|im_end|>
  <|im_start|>user
  $input<|im_end|>
  <|im_start|>assistant
  ''';
//   <start_of_turn>user
// You are a helpful, concise assistant. Please answer the following query:
// $input<end_of_turn>
// <start_of_turn>model
      await llamaParent.sendPrompt(prompt);
    }

    // âœ… Ø´Ù†ÙˆÙ†Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ§Ù† ØªÙˆÙ„ÛŒØ¯ â€” ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø±
    late StreamSubscription completionSub;
    completionSub = llamaParent.completions.listen((event) {
      if (!mounted) return;
      if (event.success) {
        setState(() {
          _isTyping = false;
        });
      } else {
        if (mounted) {
          setState(() {
            _messages.last.text += "\n\nâŒ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯.";
            _isTyping = false;
          });
        }
      }
      subscription.cancel();
      completionSub.cancel();
    });
  }

  // Submit query to server
  Future<int?> submitQuery(String query) async {
    try {
      final response = await http.post(
        Uri.parse('$routingServerUrl/query'),
        headers: {'Content-Type': 'application/json'},
        body: jsonEncode({'query': query}),
      );

      if (response.statusCode == 200) {
        final result = int.tryParse(response.body.trim());
        print("[User] âœ… query_number received: $result");
        return result;
      } else {
        print("[User] âŒ Query submission failed: ${response.statusCode} - ${response.body}");
        return null;
      }
    } catch (e) {
      print("[User] âŒ Connection error (server unreachable): $e");
      return null; // Triggers fallback
    }
  }

  // Wait for responses from other nodes
Future<List<List<String>>> waitForWorkerResponses(int queryNumber) async {
  final List<List<String>> responses = [];
  const maxWait = 30; // 25 seconds timeout
  int elapsed = 0;

  while (elapsed < maxWait) {
    try {
      final response = await http.get(
        Uri.parse('$routingServerUrl/response?query_number=$queryNumber'),
      );

      if (response.statusCode == 200) {
        final data = jsonDecode(response.body);
        if (data is List && data.isNotEmpty) {
          final converted = <List<String>>[];
          for (var item in data) {
            if (item is List) {
              converted.add(item.map((e) => e.toString()).toList());
            }
          }
          print("[User] âœ… Received responses: $converted");
          return converted;
        }
      }
    } catch (e) {
      print("[User] âŒ Error fetching responses: $e");
    }

    await Future.delayed(const Duration(seconds: 1));
    elapsed++;
  }

  print("[User] â³ Timeout waiting for worker responses.");
  return responses; // Ø®Ø§Ù„ÛŒ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
}

  // Build final prompt
  String buildFinalPrompt(String userQuery, List<List<String>> workerResponses) {
    final context = StringBuffer();
    for (var resp in workerResponses) {
      context.write(resp.join(' ') + ' ');
    }

    if (context.isEmpty) {
      return '''
Question: $userQuery
Answer:
''';
    } else {
      return '''
You are a helpful AI assistant. Answer the user's question based on the provided context from other nodes.

Context:
${context.toString().trim()}

User Question: $userQuery
Answer:
''';
    }
  }

  // Cleanup query
  Future<void> cleanupQuery(int queryNumber) async {
    try {
      await http.post(
        Uri.parse('$routingServerUrl/end'),
        headers: {'Content-Type': 'application/json'},
        body: jsonEncode({'query_number': queryNumber}),
      );
      print("[User] ğŸ§¹ Query $queryNumber cleaned up successfully.");
    } catch (e) {
      print("[User] âŒ Cleanup error: $e");
    }
  }

  @override
  void dispose() {
    _workerTimer.cancel();
    _controller.dispose();
    llamaParent.dispose();
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: const Text('AI Assistant'),
        centerTitle: false,
        backgroundColor: Colors.grey[900],
        actions: [
          IconButton(
            icon: const Icon(Icons.refresh, size: 20),
            onPressed: () {
              setState(() {
                _messages.clear();
              });
            },
          )
        ],
      ),
      body: Container(
        color: const Color(0xFF121212),
        child: Column(
          children: <Widget>[
            Expanded(
              child: ListView.builder(
                padding: const EdgeInsets.symmetric(horizontal: 16, vertical: 12),
                itemCount: _messages.length,
                itemBuilder: (context, index) {
                  final message = _messages[index];
                  return _buildMessageBubble(message);
                },
              ),
            ),
            if (_isTyping)
              Container(
                padding: const EdgeInsets.symmetric(vertical: 6, horizontal: 16),
                color: Colors.blueGrey[900],
                child: const Row(
                  mainAxisSize: MainAxisSize.min,
                  children: [
                    Text(
                      "Combining responses...",
                      style: TextStyle(
                        color: Colors.white70,
                        fontSize: 14,
                        fontStyle: FontStyle.italic,
                      ),
                    ),
                    SizedBox(width: 8),
                    SizedBox(
                      width: 16,
                      height: 16,
                      child: CircularProgressIndicator(
                        strokeWidth: 2,
                        valueColor: AlwaysStoppedAnimation<Color>(Colors.white),
                      ),
                    ),
                  ],
                ),
              ),
            _buildInputArea(),
          ],
        ),
      ),
    );
  }

  Widget _buildMessageBubble(ChatMessage message) {
    final isUser = message.isFromUser;
    final color = isUser ? Colors.blue[600]! : Colors.grey[800]!;
    final alignment = isUser ? Alignment.centerRight : Alignment.centerLeft;

    return Padding(
      padding: const EdgeInsets.symmetric(vertical: 6),
      child: Align(
        alignment: alignment,
        child: ConstrainedBox(
          constraints: BoxConstraints(maxWidth: MediaQuery.of(context).size.width * 0.7),
          child: Container(
            padding: const EdgeInsets.all(14),
            decoration: BoxDecoration(
              color: color,
              borderRadius: BorderRadius.only(
                topLeft: Radius.circular(16),
                topRight: Radius.circular(16),
                bottomLeft: isUser ? Radius.circular(16) : Radius.circular(4),
                bottomRight: isUser ? Radius.circular(4) : Radius.circular(16),
              ),
              boxShadow: const [
                BoxShadow(
                  color: Colors.black38,
                  blurRadius: 8,
                  offset: Offset(0, 4),
                ),
              ],
            ),
            child: Text(
              message.text.isEmpty && !isUser ? "..." : message.text,
              style: const TextStyle(
                color: Colors.white,
                fontSize: 15,
                height: 1.5,
                fontFamily: 'Monospace',
              ),
              textAlign: TextAlign.left,
              textDirection: TextDirection.ltr,
              softWrap: true,
              overflow: TextOverflow.visible,
            ),
          ),
        ),
      ),
    );
  }

  Widget _buildInputArea() {
    return Container(
      padding: const EdgeInsets.symmetric(horizontal: 12, vertical: 10),
      color: const Color(0xFF1E1E1E),
      child: Row(
        children: <Widget>[
          Expanded(
            child: Container(
              decoration: BoxDecoration(
                color: Colors.grey[800],
                borderRadius: BorderRadius.circular(24),
                border: Border.all(color: Colors.grey[600]!, width: 1),
              ),
              child: TextField(
                controller: _controller,
                onSubmitted: _isTyping ? null : _sendMessage,
                enabled: !_isTyping,
                style: const TextStyle(color: Colors.white, fontSize: 16),
                textDirection: TextDirection.ltr,
                decoration: const InputDecoration(
                  hintText: 'Type your message...',
                  hintStyle: TextStyle(color: Colors.grey),
                  contentPadding: EdgeInsets.symmetric(horizontal: 16, vertical: 14),
                  border: InputBorder.none,
                ),
              ),
            ),
          ),
          const SizedBox(width: 8),
          FloatingActionButton(
            onPressed: _isTyping ? null : () => _sendMessage(_controller.text),
            backgroundColor: _isTyping ? Colors.grey : Colors.blue,
            elevation: 2,
            mini: true,
            child: const Icon(Icons.send, color: Colors.white, size: 22),
          ),
        ],
      ),
    );
  }
}

class ChatMessage {
  String text;
  final bool isFromUser;
  ChatMessage({required this.text, required this.isFromUser});
}